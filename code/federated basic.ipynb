{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Re-run the full blockchain-integrated federated graph recommender using the re-uploaded file\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import networkx as nx\n",
    "# from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "# import random\n",
    "\n",
    "# # Parameters\n",
    "# NUM_CLIENTS = 5\n",
    "# TOP_K = 5\n",
    "\n",
    "# # Load dataset\n",
    "# df = pd.read_csv(\"/mnt/data/Last.fm_data.csv\")\n",
    "# df = df.rename(columns={'Username': 'user_id', 'Artist': 'item_id'})\n",
    "# df = df[['user_id', 'item_id']]\n",
    "# df['count'] = 1\n",
    "# df = df.groupby(['user_id', 'item_id']).count().reset_index()\n",
    "# df['rating'] = pd.qcut(df['count'], q=5, labels=False) + 1\n",
    "\n",
    "# # Partition users for simulation\n",
    "# users = df['user_id'].unique()\n",
    "# random.shuffle(users)\n",
    "# client_user_splits = np.array_split(users, NUM_CLIENTS)\n",
    "\n",
    "# client_datasets = []\n",
    "# for users_subset in client_user_splits:\n",
    "#     client_df = df[df['user_id'].isin(users_subset)]\n",
    "#     client_datasets.append(client_df)\n",
    "\n",
    "# # Graph-based recommender functions\n",
    "# def build_graph(df):\n",
    "#     G = nx.Graph()\n",
    "#     for _, group in df.groupby(\"user_id\"):\n",
    "#         items = group['item_id'].tolist()\n",
    "#         for i in range(len(items)):\n",
    "#             for j in range(i + 1, len(items)):\n",
    "#                 G.add_edge(items[i], items[j], weight=G.get_edge_data(items[i], items[j], {}).get('weight', 0) + 1)\n",
    "#     return G\n",
    "\n",
    "# def recommend(graph, user_df, top_k=TOP_K):\n",
    "#     user_items = set(user_df['item_id'])\n",
    "#     scores = {}\n",
    "#     for item in user_items:\n",
    "#         for neighbor in graph.neighbors(item):\n",
    "#             if neighbor not in user_items:\n",
    "#                 scores[neighbor] = scores.get(neighbor, 0) + graph[item][neighbor]['weight']\n",
    "#     return sorted(scores, key=scores.get, reverse=True)[:top_k]\n",
    "\n",
    "# # Evaluation metrics\n",
    "# def evaluate(client_df, train_graph, top_k=TOP_K):\n",
    "#     users = client_df['user_id'].unique()\n",
    "#     precisions, recalls, f1s, hits = [], [], [], []\n",
    "#     for user in users:\n",
    "#         user_df = client_df[client_df['user_id'] == user]\n",
    "#         items = user_df['item_id'].tolist()\n",
    "#         if len(items) < 2:\n",
    "#             continue\n",
    "#         random.shuffle(items)\n",
    "#         split = int(len(items) * 0.5)\n",
    "#         train_items = items[:split]\n",
    "#         test_items = items[split:]\n",
    "\n",
    "#         user_partial_df = pd.DataFrame({'user_id': [user]*len(train_items), 'item_id': train_items})\n",
    "#         recs = recommend(train_graph, user_partial_df, top_k)\n",
    "\n",
    "#         y_true = [1 if item in test_items else 0 for item in recs]\n",
    "#         y_pred = [1]*len(y_true)\n",
    "\n",
    "#         if sum(y_true) > 0:\n",
    "#             precisions.append(precision_score(y_true, y_pred, zero_division=0))\n",
    "#             recalls.append(recall_score(y_true, y_pred, zero_division=0))\n",
    "#             f1s.append(f1_score(y_true, y_pred, zero_division=0))\n",
    "#             hits.append(1 if any(y_true) else 0)\n",
    "\n",
    "#     return {\n",
    "#         'precision': np.mean(precisions) if precisions else 0,\n",
    "#         'recall': np.mean(recalls) if recalls else 0,\n",
    "#         'f1': np.mean(f1s) if f1s else 0,\n",
    "#         'hit_rate': np.mean(hits) if hits else 0\n",
    "#     }\n",
    "\n",
    "# # Token reward system\n",
    "# def reward_clients(metrics_list):\n",
    "#     f1_scores = np.array([metrics['f1'] for metrics in metrics_list])\n",
    "#     f1_scores = f1_scores / f1_scores.sum() if f1_scores.sum() > 0 else np.ones_like(f1_scores) / len(f1_scores)\n",
    "#     base_tokens = 1000\n",
    "#     return {f\"client_{i}\": round(score * base_tokens, 2) for i, score in enumerate(f1_scores)}\n",
    "\n",
    "# # Run simulation\n",
    "# all_metrics = []\n",
    "# global_graph = build_graph(df)\n",
    "# for client_df in client_datasets:\n",
    "#     metrics = evaluate(client_df, global_graph)\n",
    "#     all_metrics.append(metrics)\n",
    "\n",
    "# token_rewards = reward_clients(all_metrics)\n",
    "# metrics_df = pd.DataFrame(all_metrics)\n",
    "# metrics_df['client'] = [f'client_{i}' for i in range(NUM_CLIENTS)]\n",
    "# metrics_df['tokens'] = metrics_df['client'].map(token_rewards)\n",
    "# metrics_df.set_index('client', inplace=True)\n",
    "# metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import random\n",
    "\n",
    "# Simulate 5 clients\n",
    "NUM_CLIENTS = 5\n",
    "TOP_K = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\anuna\\Downloads\\Last.fm_data.csv\\Last.fm_data.csv\")\n",
    "df = df.rename(columns={'Username': 'user_id', 'Artist': 'item_id'})\n",
    "df = df[['user_id', 'item_id']]\n",
    "df['count'] = 1\n",
    "df = df.groupby(['user_id', 'item_id']).count().reset_index()\n",
    "df['rating'] = pd.qcut(df['count'], q=5, labels=False, duplicates='drop') + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition users for simulation\n",
    "users = df['user_id'].unique()\n",
    "random.shuffle(users)\n",
    "client_user_splits = np.array_split(users, NUM_CLIENTS)\n",
    "\n",
    "client_datasets = []\n",
    "for users_subset in client_user_splits:\n",
    "    client_df = df[df['user_id'].isin(users_subset)]\n",
    "    client_datasets.append(client_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph-based recommender functions\n",
    "def build_graph(df):\n",
    "    G = nx.Graph()\n",
    "    for _, group in df.groupby(\"user_id\"):\n",
    "        items = group['item_id'].tolist()\n",
    "        for i in range(len(items)):\n",
    "            for j in range(i + 1, len(items)):\n",
    "                G.add_edge(items[i], items[j], weight=G.get_edge_data(items[i], items[j], {}).get('weight', 0) + 1)\n",
    "    return G\n",
    "\n",
    "def recommend(graph, user_df, top_k=TOP_K):\n",
    "    user_items = set(user_df['item_id'])\n",
    "    scores = {}\n",
    "    for item in user_items:\n",
    "        for neighbor in graph.neighbors(item):\n",
    "            if neighbor not in user_items:\n",
    "                scores[neighbor] = scores.get(neighbor, 0) + graph[item][neighbor]['weight']\n",
    "    return sorted(scores, key=scores.get, reverse=True)[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics\n",
    "def evaluate(client_df, train_graph, top_k=TOP_K):\n",
    "    users = client_df['user_id'].unique()\n",
    "    precisions, recalls, f1s, hits = [], [], [], []\n",
    "    for user in users:\n",
    "        user_df = client_df[client_df['user_id'] == user]\n",
    "        items = user_df['item_id'].tolist()\n",
    "        if len(items) < 2:\n",
    "            continue\n",
    "        random.shuffle(items)\n",
    "        split = int(len(items) * 0.5)\n",
    "        train_items = items[:split]\n",
    "        test_items = items[split:]\n",
    "\n",
    "        user_partial_df = pd.DataFrame({'user_id': [user]*len(train_items), 'item_id': train_items})\n",
    "        recs = recommend(train_graph, user_partial_df, top_k)\n",
    "\n",
    "        y_true = [1 if item in test_items else 0 for item in recs]\n",
    "        y_pred = [1]*len(y_true)\n",
    "\n",
    "        if sum(y_true) > 0:\n",
    "            precisions.append(precision_score(y_true, y_pred, zero_division=0))\n",
    "            recalls.append(recall_score(y_true, y_pred, zero_division=0))\n",
    "            f1s.append(f1_score(y_true, y_pred, zero_division=0))\n",
    "            hits.append(1 if any(y_true) else 0)\n",
    "\n",
    "    return {\n",
    "        'precision': np.mean(precisions) if precisions else 0,\n",
    "        'recall': np.mean(recalls) if recalls else 0,\n",
    "        'f1': np.mean(f1s) if f1s else 0,\n",
    "        'hit_rate': np.mean(hits) if hits else 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token reward system\n",
    "def reward_clients(metrics_list):\n",
    "    f1_scores = np.array([metrics['f1'] for metrics in metrics_list])\n",
    "    f1_scores = f1_scores / f1_scores.sum() if f1_scores.sum() > 0 else np.ones_like(f1_scores) / len(f1_scores)\n",
    "    base_tokens = 1000\n",
    "    return {f\"client_{i}\": round(score * base_tokens, 2) for i, score in enumerate(f1_scores)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>client</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>client_0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>client_1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>client_2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>client_3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>client_4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          precision  recall   f1  hit_rate  tokens\n",
       "client                                            \n",
       "client_0        1.0     1.0  1.0       1.0   200.0\n",
       "client_1        1.0     1.0  1.0       1.0   200.0\n",
       "client_2        1.0     1.0  1.0       1.0   200.0\n",
       "client_3        1.0     1.0  1.0       1.0   200.0\n",
       "client_4        1.0     1.0  1.0       1.0   200.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run simulation\n",
    "all_metrics = []\n",
    "global_graph = build_graph(df)\n",
    "for client_df in client_datasets:\n",
    "    metrics = evaluate(client_df, global_graph)\n",
    "    all_metrics.append(metrics)\n",
    "\n",
    "token_rewards = reward_clients(all_metrics)\n",
    "metrics_df = pd.DataFrame(all_metrics)\n",
    "metrics_df['client'] = [f'client_{i}' for i in range(NUM_CLIENTS)]\n",
    "metrics_df['tokens'] = metrics_df['client'].map(token_rewards)\n",
    "metrics_df.set_index('client', inplace=True)\n",
    "metrics_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
